{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Insegnare a un computer a riconoscere immagini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Ma prima di tutto...\n",
    "\n",
    "Esegui la cella qui sotto, senza preoccuparti pi√π di tanto di che cosa succede: serve solo a rendere disponibile del codice che abbiamo scritto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import ice\n",
    "except ModuleNotFoundError:\n",
    "    import sys\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        !python -m pip install -qqq --upgrade -- uv && python -m uv pip install --system --quiet -- https://github.com/baggiponte/aiss-2024.git\n",
    "        !mkdir public && curl -L https://github.com/baggiponte/aiss-2024/raw/main/public/02-nn_training.mp4 -o public/02-nn_training.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# MNIST: un dataset fatto di cifre scritte a mano"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Cominciamo con uno dei dataset pi√π famosi della storia delle reti neurali. [MNIST](https://en.wikipedia.org/wiki/MNIST_database) √® un dataset di settantamila cifre da 0 a 9 scritte a mano e digitalizzate in un formato da 28x28 pixel, in bianco e nero (scala di grigio). Viene usato in un articolo piuttosto famoso, firmato da diversi \"padri\" del deep learning. Per chi fosse interessato, [qui](https://www.youtube.com/watch?v=mTtDfKgLm54&list=PLLHTzKZzVU9e6xUfG10TkTWApKSZCzuBI) c'√® la prima lezione di un corso di Deep Learning della New York University: forse non √® il caso di seguirlo tutto, ma se ve la cavate con l'inglese potete sentire Yann LeCunn, uno degli autori del paper di MNIST e attuale Chief AI Scientist di Meta (cio√® il capo di tutta la parte di AI di Facebook, Instagram, ecc), raccontare la storia del deep learning.\n",
    "\n",
    "Useremo anche noi questo dataset per allenare la nostra prima rete neurale. Fun fact: il tasso d'errore pi√π basso raggiunto per MNIST √® circa dello 0.2/0.3%. Quanto riusciremo ad avvicinarci?\n",
    "\n",
    "Prima di costruire, esegui il contenuto della cella seguente e usa lo slider per esplorare le fotografie. Potrebbero esserci delle foto etichettate male: alcune fonti riportano che siano almeno 4 (che pu√≤ essere il motivo per cui, di fatto, √® impossibile ottenere il 100% di accuratezza, a meno di correzioni)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ice.utils import load_mnist, display\n",
    "\n",
    "train, test = load_mnist(path=\"../data\")\n",
    "\n",
    "display(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "L'immagine che vediamo √® semplicemente una matrice di numeri: possiamo vederlo cos√¨:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# questa √® un po' di magia nera, ma non √® niente di difficile\n",
    "# nota che `[i]` viene usato per accedere all'elemento i-esimo di una lista/contenitore\n",
    "image, label = train[0]\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Vista cos√¨ la forma non ha molto senso (solo perch√© la matrice non √® allineata in 28x28), ma si nota molto bene di che cosa √® fatta: una grossa matrice con valori che vanno da 0 (nero) a 255 (bianco).\n",
    "\n",
    "Gi√† a occhio nudo, comunque, possiamo vedere dei pattern: il processo di allenamento di una rete neurale √® una metodo che abbiamo affinato per far s√¨ che un computer impari a riconoscerli e sfruttarli per associare un input a un determinato output. Vediamo come!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# La struttura di una rete neurale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Prima di mettere mano alla tastiera, per√≤, proviamo a spiegare un po' come funzionano le reti neurali."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Come funzionano le reti neurali?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Non scendiamo nel dettaglio di come funziona una rete neurale: non perch√© sia difficile, ma perch√© sono necessari alcuni concetti. In linea di massima, nel caso di un problema di classificazione come il nostro, una rete neurale fa quello che si vede in [questo spezzone](https://youtu.be/mTtDfKgLm54?si=STy0SyJe17i73_Ag&t=2809) della lezione che abbiamo condiviso sopra.\n",
    "\n",
    "Si comincia da dei punti nello spazio che non sono *linearmente separabili*, cio√® non si possono separare con una o pi√π linee rette:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"../public/01-blob.png\" style=\"width: 300px; height: auto;\">\n",
    "</div>\n",
    "\n",
    "L'algoritmo \"apprende\" quali sono le trasformazioni dello spazio (distorsioni, traslazioni, e simili) per isolare sottoinsiemi nei dati, finch√© non diventano separabili con una linea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"./public/02-nn_training.mp4\", width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "In altre parole, semplificando un po', √® come se il modello dovesse imparare a tracciare delle linee storte:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"../public/03-blob_separated.png\" style=\"width: 300px; height: auto;\">\n",
    "</div>\n",
    "\n",
    "In che senso \"impara\"? In parole povere, comincia con delle trasformazioni a caso, genera una predizione, giusta o sbagliata, e riceve un \"feedback\" a ogni iterazione su come aggiustare il tiro. A un certo punto, quando l'errore √® accettabile, interrompiamo l'allenamento.\n",
    "\n",
    "Se vi ricordate l'equazione della retta: \n",
    "\n",
    "$y = mx + q$\n",
    "\n",
    "Ecco, una rete neurale non ha solo un parametro $m$ e $q$, ma ne ha solitamente da milioni a miliardi. L'obiettivo dell'allenamento √®, partendo da $x$, trovare dei parametri (detti anche pesi) abbastanza precisi da trovare la $y$ con un margine d'errore accettabile. Come funziona \"l'aggiustamento\"? √à una procedura *iterativa*, cio√® ripetuta:\n",
    "\n",
    "1. usando i pesi attuali si genera una predizione;\n",
    "2. si misura l'errore della predizione;\n",
    "3. infine si usa l'errore per modificare leggermente i pesi;\n",
    "\n",
    "E via daccapo dal punto 1. Per chi l'avesse visto, la procedura pi√π semplice - e quella che useremo noi - √® simile al metodo di Newton. In altre parole, si usano le derivate.\n",
    "\n",
    "Ci sono molte introduzioni ben fatte (praticamente solo in inglese). Qui una carrellata:\n",
    "\n",
    "1. [MLU Explain](https://mlu-explain.github.io/neural-networks/). Spiegazione completa e interattiva.\n",
    "2. [3Blue1Brow](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi). Sicuramente pi√π densa ma non per questo difficile da capire. Tenetelo come riferimento per vari spiegoni di matematica (per il quinto anno ma anche per l'universit√†): non solo spiega bene, ma fa anche capire il significato e come interpretare certi concetti di matematica, probabilit√† e statistica.\n",
    "3. Con un po' pi√π di dettaglio [introduzione](https://www.youtube.com/watch?v=UOvPeC8WOt8) e poi [parte 2](https://www.youtube.com/watch?v=-at7SLoVK_I)\n",
    "\n",
    "Lasciamoci alle spalle la teoria, e andiamo alla pratica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Disegnare una rete neurale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Per quanto semplice, la spiegazione delle reti neurali deve essere gi√† stata un po' densa. Per fortuna, in Python non dobbiamo occuparci noi di programare niente di tutto questo! `pytorch` √® una libreria (o *framework*) che ci permette di disegnare e allenare reti neurali rapidamente.\n",
    "\n",
    "Basta una decina di righe per definirne una:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layer = nn.Linear(28 * 28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_flat = self.flatten(x)\n",
    "        logits = self.layer(x_flat)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Ci sono comunque un po' di cose da spacchettare, quindi andiamo con ordine.\n",
    "\n",
    "Innanzitutto: che cosa c'√® scritto? Stiamo definendo una *classe*, cio√® creiamo un particolare tipo di oggetto. Questo oggetto *eredita* da `nn.Module`. Un oggetto - lo abbiamo visto nell'introduzione - √® un contenitore di \"attributi\" e \"comportamenti\". Per fare un esempio: un oggetto `Cane` potrebbe avere come \"attributi\" il suo nome, il colore, il cibo preferito... E come \"comportamenti\" (detti anche *metodi*) abbaiare, rincorrere, recuperare un bastone. Non ci serve sapere molto altro degli oggetti, n√© come crearli, salvo tenere presente che per costurire una rete funzionante dobbiamo definire due cose:\n",
    "\n",
    "1. La struttura della nostra rete, che facciamo nel metodo `__init__`\n",
    "2. Come generare una previsione, con il metodo `forward`.\n",
    "\n",
    "In altre parole, tutto quello su cui dobbiamo concentrarci √®:\n",
    "\n",
    "```python\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layer = nn.Linear(28 * 28, 10)\n",
    "```\n",
    "\n",
    "Questa √® la struttura della nostra rete (gli \"strati\" del \"panino\"). Sono degli attributi che abbiamo chiamato `flatten` e `layer`.\n",
    "\n",
    "1. Il primo \"strato\" (in inglese, *layer*) che appiattisce (*flatten*) la fotografia da una matrice 28*28 a un'unico vettore di lunghezza 784. Non dobbiamo modificarlo.\n",
    "2. Il secondo e ultimo strato √® un oggetto particolare, `nn.Linear()`. Di fatto, √® un modo compatto per scrivere i coefficienti del modello, un po' come se fosse `$y = mx + q$`. `nn.Linear(28 * 28, 10)` prende tutti i nostri pixel di input e li \"rimappa\" direttamente a 10 valori. In altre parole, √® come se disegnassimo una retta che prende i nostri pixel e cerca di restituire un vettore che ci dice qual √® il numero pi√π probabile.\n",
    "3. Nel `forward` step vediamo proprio come viene usata la rete neurale per generare le predizioni (dette *logits*). √à un'altro pezzo che non dobbiamo modificare.\n",
    "\n",
    "Possiamo vedere direttamente come √® fatto questo oggetto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Questa rete neurale, per√≤, non funzioner√† molto bene. √à molto semplice e le manca un elemento importante: la possibilit√† di imparare trasformazioni *non* lineari. Per questo aggiungiamo al panino uno strato \"speciale\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_flat = self.flatten(x)\n",
    "        logits = self.stack(x_flat)\n",
    "        return logits\n",
    "\n",
    "model = MLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Per comodit√†, usiamo `nn.Sequential` per raggruppare i layer, ma di per s√© non fa niente di speciale. Ora la rete ha tre strati:\n",
    "\n",
    "1. Una mappatura da 784 dimensioni in input a una \"intermedia\" di 128 elementi - i cosiddetti \"neuroni\".\n",
    "2. Uno strato intermedio che trasforma lo spazio in modo non lineare.\n",
    "3. L'ultimo strato, dello stesso tipo del primo, che mappa da 128 a 10 neuroni - il nostro output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "# Allenare un modello\n",
    "\n",
    "Eccoci nella parte complicata - per il computer, per√≤. Abbiamo scritto per te due oggetti, un `TrainerConfig` e un `Trainer`, per semplificarti il lavoro.\n",
    "\n",
    "* `Trainer` √® un oggetto che si occupa di allenare il modello per te e di dirti come sta procedendo l'allenamento.\n",
    "* `TrainerConfig` contiene le configurazioni del `Trainer`.\n",
    "\n",
    "Per cominciare, esegui il codice qui sotto. Quando il `trainer` ha finito di allenare il modello, qual √® l'accuratezza media alla fine dell'allenamento?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ice.trainer import TrainerConfig, Trainer\n",
    "\n",
    "config = TrainerConfig(\n",
    "    epochs=3,\n",
    "    batch_size=16,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, config=config, dataset=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Come vedete, per√≤, le prestazioni lasciano a desiderare. Forse √® il caso di fare di pi√π: pensi di potercela fare? Qualche consiglio:\n",
    "\n",
    "Modificare la rete neurale:\n",
    "\n",
    "1. Attenzione, per√≤: non cambiare mai il numero di dimensioni iniziale (`28 * 28`) e finale (`10`), altrimenti il modello non potr√† pi√π prendere in input le immagini di MNIST, o assegnare loro la categoria.\n",
    "2. Alterna sempre un `nn.Linear(...)` con `nn.ReLU()`. Puoi anche usare `nn.Softmax()` o `nn.Tanh()`.\n",
    "3. Puoi mettere tutti i neuroni che vuoi negli strati intermedi, e anche un numero a piacere di strati. Pi√π strati e neuroni ci sono, pi√π il modello ci metter√† tempo ad allenare, anche se avr√† migliori prestazioni.\n",
    "4. Non ci sono regole per il numero di neuroni: puoi provare tanti strati con lo stesso numero, o ridurli progressivamente, o ancora prima accrescere e poi ridurre ancora. Puoi mettere pi√π neuroni dei 784 input.\n",
    "5. Per consuetudine, usiamo numeri che sono potenze di 2 (64, 128, 256, 512, ecc) perch√© cos√¨ il computer riesce a fare le moltiplicazioni pi√π velocemente, e quindi il modello si allena prima. Potete cambiare di poco (510, 520...) ma consigliamo di non provare numeri troppo strani.\n",
    "\n",
    "Modificare il parametri del Trainer:\n",
    "1. Allenare per pi√π epoche.\n",
    "2. Cambiare le dimensioni della `batch_size`. Di solito le batch non sono troppo grandi (solitamente non pi√π di 128) ma potete provare tutte le potenze di due nel mezzo. Pi√π la batch √® grande e pi√π il modello si allena velocemente.\n",
    "3. Cambiare il `learning_rate`. Attenzione: un learning rate pi√π basso significa che il modello ci mette pi√π tempo ad allenare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "# Come valutare la qualit√† di un modello?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "I pi√π attenti avranno notato che, quando abbiamo caricato i dati, abbiamo sia un `train` che un `test`. Si tratta di una pratica cruciale per chi fa machine learning: tenere da parte un pezzo del dataset e non usarlo per allenare il modello. Viene messo da parte, dimenticato e sotto chiave, finch√© il modello non √® pronto. Si usa infine il modello per comparare le prestazioni su dati che non ha visto prima. Per cui: quando siete convinti che il vostro modello sia adatto, e solo allora, eseguite le celle qui sotto e vedete come √® andata üòé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ice.eval import predict, evaluate\n",
    "\n",
    "predict(model=model, dataset=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Percentage: {evaluate(model=model, dataset=test):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "# E le foto degli animali?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Non ce ne siamo dimenticati! Ma √® un problema che richiede molte risorse e modelli pi√π sofisticati: praticamente √® certo che le architetture che vi abbiamo mostrato non sono sufficienti. Qualche considerazione:\n",
    "\n",
    "1. Sono foto a colori, e cio√® hanno tre canali (RGB, e cio√® rosso, verde e blu), mentre le cifre di MNIST erano in bianco e nero. In altre parole: a parit√† di dimensioni, hanno 3 volte tanti pixel. L'input layer del nostro modello, quindi, avrebbe 784 * 3 = 2352 neuroni.\n",
    "2. In realt√†, per√≤, queste immagini hanno dimensioni pi√π grandi. La pi√π piccola √® di 120 pixel di larghezza, la pi√π grande circa quattromila. Per allenare un modello, dobbiamo per forza ridimensionare le immagini per avere tutte quante le stesse dimensioni. Si finisce, per√≤, per forza di cose, a perdere informazioni. Ma anche con delle foto 122 * 122, avremmo un input layer di 44.652 neuroni. Quasi 100 volte pi√π grande di quello che abbiamo allenato finora, e stiamo parlando solo dell'input layer!\n",
    "3. Le foto sono solo 6000. Dovremmo tenerne da parte almeno un 20% per il test set, quindi lavoreremmo con 4800 foto. Il solo training set di MNIST ne ha 60000, 10 volte di pi√π. Allenare un modello 3-4 ordini di grandezza pi√π grande (100-1000 volte pi√π grande) richiede di aumentare i dati di una quantit√† simile. Provate ad allenare lo stesso modello con solo 600 foto e vedete se le prestazioni sul test set sono le stesse: probabilmente s√¨ (il dataset non √® molto variegato) ma sar√† difficile passare dal 90% al 92% di accuratezza.\n",
    "\n",
    "Che cosa possiamo fare? Una soluzione c'√®."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
